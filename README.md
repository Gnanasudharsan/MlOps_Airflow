# MlOps_Airflow

# Airflow Lab 1 ‚Äì Heart Disease Clustering (DBSCAN)

This project implements an **Apache Airflow pipeline** for automating an end-to-end **data processing and unsupervised machine learning workflow**.  
The DAG performs **ETL, preprocessing, DBSCAN clustering, and model evaluation** on the **UCI Heart Disease dataset** using Airflow‚Äôs task orchestration capabilities.

---

## üéØ Objective
To build a modular, automated pipeline in Airflow that:
1. Loads and cleans the heart disease dataset  
2. Scales and encodes data for clustering  
3. Builds and saves a **DBSCAN clustering model**  
4. Evaluates model quality using the **Silhouette Score**

---

## ‚öôÔ∏è Airflow DAG Overview
The DAG is named **`Airflow_Lab1_HeartDisease`** and includes the following tasks:

| Task | Description | Output |
|------|--------------|--------|
| **load_data** | Loads the UCI Heart Disease dataset and drops null values | `processed_heart_disease.csv` |
| **data_preprocessing** | Encodes categorical features and scales numerical features using `MinMaxScaler` | `scaled_heart_disease.csv` |
| **build_save_model** | Builds a **DBSCAN** clustering model and saves it using `pickle` | `dbscan_model.pkl` |
| **evaluate_model** | Evaluates clustering quality using **Silhouette Score** | Printed in logs |

Each task runs as an **independent PythonOperator** inside Airflow.

---

## üß† ML Model Used ‚Äì DBSCAN
- **Model:** Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  
- **Reason:** Unlike K-Means, DBSCAN doesn‚Äôt require specifying the number of clusters. It‚Äôs effective for irregular, non-spherical data distributions.  
- **Evaluation Metric:** Silhouette Score (higher = better cluster separation)

---

## üß∞ Prerequisites
Before running the project, ensure you have:
- Docker Desktop (‚â• 4 GB memory allocated)
- Git
- Internet access to pull the Airflow image

Python libraries (installed inside the container):
```bash
pandas
scikit-learn
pickle
numpy
apache-airflow==2.5.1

```
‚∏ª

üöÄ Setup and Execution

1Ô∏è‚É£ Clone the Repository
``` bash
git clone https://github.com/Gnanasudharsan/MlOps_Airflow.git
cd MlOps_Airflow
```
2Ô∏è‚É£ Start Airflow in Docker
``` bash
docker compose up -d
```
Wait until you see the message:
```bash
airflow-webserver | 127.0.0.1 - - "GET /health HTTP/1.1" 200 -
```
3Ô∏è‚É£ Access Airflow UI

Visit http://localhost:8080
Login using:
``` bash
Username: airflow
Password: airflow
```
4Ô∏è‚É£ Trigger the DAG
	‚Ä¢	Go to DAGs ‚Üí Airflow_Lab1_HeartDisease
	‚Ä¢	Turn it ON
	‚Ä¢	Click Trigger DAG
	‚Ä¢	Monitor progress under the Graph View

When successful, all tasks will appear green ‚úÖ:
``` bash
load_data ‚Üí data_preprocessing ‚Üí build_save_model ‚Üí evaluate_model
```

‚∏ª

üìä Output Files

After successful DAG run:

File	Location	Description
processed_heart_disease.csv	/opt/airflow/dags/data	Cleaned dataset
scaled_heart_disease.csv	/opt/airflow/dags/data	Normalized dataset
dbscan_model.pkl	/opt/airflow/dags/models	Trained DBSCAN model

You can verify via Docker:
``` bash
docker exec -it lab_1-airflow-webserver-1 bash
ls /opt/airflow/dags/data
ls /opt/airflow/dags/models
```

‚∏ª

üß© Key Learnings
	‚Ä¢	Automated end-to-end ML workflow using Airflow DAGs
	‚Ä¢	Data preprocessing with scaling and encoding
	‚Ä¢	Clustering with DBSCAN and model persistence using Pickle
	‚Ä¢	Integration of ML lifecycle within containerized Airflow setup

‚∏ª

üß± References
	‚Ä¢	Apache Airflow Documentation
	‚Ä¢	UCI Heart Disease Dataset
